# Minimal LibreChat YAML config for using the Local AI Gateway as a custom OpenAI endpoint.
# Copy to: /var/lib/librechat/app/librechat.yaml (seeded by install.sh)

version: 1.2.1

interface:
  endpointsMenu: true
  modelSelect: true
  parameters: true
  sidePanel: true
  presets: true
  # Security hardening: hide/disable MCP server UI by default.
  # Re-enable intentionally when you want MCP-based tools.
  mcpServers:
    use: false
    create: false
    share: false
    public: false

# Security hardening: disable Actions and block MCP remote transports by default.
# To enable later, set explicit allowlists (see ai-infra/services/librechat/README.md).
actions:
  allowedDomains: []

mcpSettings:
  allowedDomains: []

endpoints:
  custom:
    - name: 'gateway'
      apiKey: '${GATEWAY_BEARER_TOKEN}'
      # Use localhost by default. Many gateway deployments bind to 127.0.0.1
      # (LAN access is meant to be via LibreChat on port 3080, not directly to 8800).
      # If LibreChat is on a different host than the gateway, change this to the gateway host/IP.
      baseURL: 'http://127.0.0.1:8800/v1'
      models:
        default: ['auto']
        # Prefer a static model list so LibreChat doesn't depend on /v1/models.
        # If your gateway implements /v1/models, you can set this to true.
        fetch: false
      modelDisplayLabel: 'gateway'
